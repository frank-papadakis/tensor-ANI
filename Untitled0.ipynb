{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFipB5dgCdRR",
        "outputId": "9fd7bc2b-2341-4444-def1-0ed8c1caf825"
      },
      "source": [
        "!pip install h5py #dependency of pyanitools\n",
        "!pip install tqdm #to time operations"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6lZjWwWKzWw",
        "outputId": "517ee570-206a-474b-b24d-692686fc3586"
      },
      "source": [
        "!git clone https://github.com/fipw/tensor-ANI/\n",
        "%cd tensor-ANI\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tensor-ANI'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 19 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n",
            "/content/tensor-ANI\n",
            "ani_gdb_s01.h5\tenergies_s01.npy  readers    species_s01.npy\n",
            "coords_s01.npy\tLICENSE\t\t  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev_Mnu04Ci7T",
        "outputId": "1c6bc3f1-7a5b-48f2-cdf0-1064ab7ebd54"
      },
      "source": [
        "from readers.lib import pyanitools as pya \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "os.listdir('.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ani_gdb_s01.h5',\n",
              " 'coords_s01.npy',\n",
              " 'README.md',\n",
              " 'energies_s01.npy',\n",
              " '.git',\n",
              " 'species_s01.npy',\n",
              " 'LICENSE',\n",
              " 'readers']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxFiqupGEk6O"
      },
      "source": [
        "Code to generate datasets for the species and the coordinates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h-CnoNS6gLu",
        "outputId": "a28c49ad-cc99-4929-e836-ece5088659c4"
      },
      "source": [
        "adl = pya.anidataloader('./ani_gdb_s01.h5')\n",
        "\n",
        "def sym_to_num(species):\n",
        "    newspecies = []\n",
        "    \"\"\"\n",
        "    DICTIONARY CAN BE MODIFIED LATER, -1 is used for padding\n",
        "    \"\"\"\n",
        "    dict = {'H':0, 'O':1, 'C':2, 'N':3}\n",
        "    for s in species:\n",
        "        newspecies.append(dict[s])\n",
        "    return newspecies\n",
        "\n",
        "def pad_species(species, max_s):\n",
        "    newspecies = []\n",
        "    for i in range(max_s):\n",
        "        if i < len(species):\n",
        "            newspecies.append(species[i])\n",
        "        else:\n",
        "            newspecies.append(-1)\n",
        "    return newspecies\n",
        "\n",
        "def pad_coordinates(coords, max_s):\n",
        "    padded_coords = np.empty(shape=(0,max_s,3))\n",
        "    for conform in coords:\n",
        "        pads = np.empty(shape=(max_s,3))\n",
        "        for i in range(max_s):\n",
        "            if i < conform.shape[0]:\n",
        "                pads[i] = conform[i]\n",
        "            else:\n",
        "                pads[i] = np.array([0.,0.,0.])#pads coordinates mapped to non-existing species to 0\n",
        "        #print(padded_coords)\n",
        "        #print(pads)\n",
        "        padded_coords = np.concatenate((padded_coords, pads.reshape(-1, max_s, 3)), axis=0)\n",
        "    return padded_coords\n",
        "    \n",
        "max_s = 20 #should be determined between files, but i dont want to rewrite, see todo\n",
        "for data in adl:\n",
        "    S = data['species']\n",
        "    max_s = len(S) if len(S) > max_s else max_s\n",
        "\n",
        "species = np.empty(shape=(0,max_s))\n",
        "coordinates = np.empty(shape = (0,max_s, 3))\n",
        "energies = np.array([])\n",
        "for data in tqdm(adl):\n",
        "    P = np.array(data['coordinates'])\n",
        "    conform_num = P.shape[0]\n",
        "    newspecies = pad_species(sym_to_num(data['species']), max_s)\n",
        "    species = np.concatenate((species, np.tile(newspecies, (conform_num, 1))), axis=0)\n",
        "    #print(np.tile(newspecies, (conform_num, 1)).shape)\n",
        "    coordinates = np.concatenate((coordinates, pad_coordinates(P, max_s)), axis=0)\n",
        "    energies = np.append(energies, data['energies'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tensor-ANI/readers/lib/pyanitools.py:47: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  self.store = h5py.File(store_file)\n",
            "/content/tensor-ANI/readers/lib/pyanitools.py:59: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "  dataset = np.array(item[k].value)\n",
            "3it [00:02,  1.29it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VOyBaoVAY64"
      },
      "source": [
        "np.save('./coords_s01.npy', coordinates)\n",
        "np.save('./species_s01.npy', species)\n",
        "np.save('./energies_01.npy', energies)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8P-RYfNVEqj"
      },
      "source": [
        "Code to transform the coordinates into AEVs for every atom in a given conformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjjP3xDSVEqk"
      },
      "source": [
        "Rcr = 5.2\n",
        "Rca = 3.5\n",
        "EtaR = np.array([1.6])\n",
        "ShfR = np.linspace(0.9, Rcr, 16+1)[:-1]\n",
        "Zeta = np.array([3.2])\n",
        "ShfA = np.linspace(0.9, Rca, 8+1)[:-1]\n",
        "EtaA = np.array([8.0])\n",
        "ShfZ = (np.linspace(0, np.pi, 16+1)[:-1] +np.pi/8.0)[:-1]\n",
        "\n",
        "#reshape the hyperparameters to be ready to broadcast\n",
        "EtaR, ShfR = EtaR.reshape(-1, 1), ShfR.reshape(1,-1)\n",
        "EtaA, Zeta, ShfA, ShfZ = EtaA.reshape(-1, 1 ,1 ,1), Zeta.reshape(1,-1,1,1), ShfA.reshape(1,1,-1,1), ShfZ.reshape(1,1,1,-1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APrs6GoyVEqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97a7128f-8adb-42dd-b767-bc42f2602e29"
      },
      "source": [
        "radial_sublength = EtaR.size*ShfR.size\n",
        "radial_length = 4*radial_sublength\n",
        "angular_sublength = Zeta.size*ShfA.size*ShfZ.size*EtaA.size\n",
        "angular_length = (4*(4+1)//2)*angular_sublength\n",
        "aev_length = radial_length + angular_length\n",
        "sizes = [4, radial_sublength, radial_length, angular_sublength, angular_length]\n",
        "constants = [Rcr, EtaR, ShfR, Rca, ShfZ, EtaA, Zeta, ShfA]\n",
        "print(aev_length)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1264\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtU66NNSCwZG"
      },
      "source": [
        "def fc(dist, cut):\n",
        "    return 0.5*np.cos(dist*np.pi/cut) + 0.5\n",
        "\n",
        "def radial_terms(R_cr, eta_r, R_sr, dist):\n",
        "    dist = dist.reshape(-1, 1, 1)\n",
        "    ret = np.exp(-eta_r*(dist - R_sr)**2)*fc(dist, R_cr)\n",
        "    # At this point, ret now has shape\n",
        "    # (conformations x atoms, ?, ?, ?, ?) where ? depend on constants.\n",
        "    # We then should flat the last 4 dimensions to view the subAEV as a two\n",
        "    # dimensional tensor (onnx doesn't support negative indices in flatten)\n",
        "    return ret.reshape(ret.shape[0], -1)\n",
        "\n",
        "def angular_terms(R_ca, theta_s, eta_a, zeta, R_sa, vectors12):\n",
        "    vectors12 = vectors12.reshape(2, -1, 3, 1, 1, 1, 1)\n",
        "    distances12 = np.linalg.norm(vectors12, 2, axis=-5)\n",
        "    cos_angles = vectors12.prod(0).sum(1) / np.clip(np.prod(distances12, axis=0), a_min=1e-10, a_max=None)\n",
        "    # 0.95 is multiplied to the cos values to prevent acos from returning NaN.\n",
        "    angles = np.arccos(0.95 * cos_angles)\n",
        "    fcj12 = fc(distances12, R_ca)\n",
        "    factor1 = ((1 + np.cos(angles - theta_s)) / 2) ** zeta\n",
        "    factor2 = np.exp(-eta_a * (distances12.sum(0) / 2 - R_sa) ** 2)\n",
        "    ret = 2 * factor1 * factor2 * fcj12.prod(0)\n",
        "    # At this point, ret now has shape\n",
        "    # (conformations x atoms, ?, ?, ?, ?) where ? depend on constants.\n",
        "    # We then should flat the last 4 dimensions to view the subAEV as a two\n",
        "    # dimensional tensor (onnx doesn't support negative indices in flatten)\n",
        "    return ret.reshape(ret.shape[0], -1)\n",
        "  \n",
        "def neighbor_pairs_nopbc(padding_mask, coordinates, cutoff):\n",
        "    \"\"\"Compute pairs of atoms that are neighbors (doesn't use PBC)\n",
        "    This function bypasses the calculation of shifts and duplication\n",
        "    of atoms in order to make calculations faster\n",
        "    Arguments:\n",
        "        padding_mask (:class:`torch.Tensor`): boolean tensor of shape\n",
        "            (molecules, atoms) for padding mask. 1 == is padding.\n",
        "        coordinates (:class:`torch.Tensor`): tensor of shape\n",
        "            (molecules, atoms, 3) for atom coordinates.\n",
        "        cutoff (float): the cutoff inside which atoms are considered pairs\n",
        "    \"\"\"\n",
        "    #print(np.argwhere(padding_mask))\n",
        "    coordinates[padding_mask.nonzero()] = np.nan#possibly bad? make a soft copy\n",
        "    #print(coordinates)\n",
        "    num_atoms = padding_mask.shape[1]\n",
        "    num_mols = padding_mask.shape[0]\n",
        "    p12_all = np.triu_indices(num_atoms, 1, num_atoms)\n",
        "    p12_all_flattened = np.asarray(p12_all).flatten()\n",
        "    pair_coordinates = np.take(coordinates,p12_all_flattened, axis=1).reshape(num_mols,2,-1,3)\n",
        "    distances = np.linalg.norm((pair_coordinates[:, 0, ...] - pair_coordinates[:, 1, ...]),2,axis=-1)\n",
        "    in_cutoff = (distances <= cutoff).nonzero()\n",
        "    molecule_index, pair_index = in_cutoff\n",
        "    molecule_index *= num_atoms\n",
        "    atom_index12 = np.array(p12_all)[:, pair_index] + molecule_index\n",
        "    return atom_index12\n",
        "\n",
        "def triple_by_molecule(atom_index12):\n",
        "    # convert representation from pair to central-others\n",
        "    #print(atom_index12)\n",
        "    ai1 = atom_index12.flatten()\n",
        "    rev_indices  = ai1.argsort() \n",
        "    sorted_ai1 = ai1[rev_indices]\n",
        "    # sort and compute unique key\n",
        "    uniqued_central_atom_index, counts = np.unique(sorted_ai1, return_inverse=False, return_counts=True)\n",
        "    # compute central_atom_index\n",
        "    pair_sizes = counts * (counts - 1) // 2\n",
        "    pair_indices = np.repeat(np.arange(0, len(pair_sizes)), pair_sizes)\n",
        "    central_atom_index = uniqued_central_atom_index.take(pair_indices,0)\n",
        "    # do local combinations within unique key, assuming sorted\n",
        "    m = counts.max() if counts.size > 0 else 0\n",
        "    n = pair_sizes.shape[0]\n",
        "    intra_pair_indices = np.expand_dims(np.tril_indices(m, -1, m), axis=1)\n",
        "    intra_pair_indices = np.resize(intra_pair_indices,new_shape=(intra_pair_indices.shape[0], n, intra_pair_indices.shape[2]))\n",
        "    mask = (np.arange(intra_pair_indices.shape[2]) < np.expand_dims(pair_sizes, axis=1)).flatten()\n",
        "    sorted_local_index12 = intra_pair_indices.reshape(intra_pair_indices.shape[0], -1)[:, mask]\n",
        "    sorted_local_index12 += np.take(cumsum_from_zero(counts), pair_indices,axis=0)\n",
        "\n",
        "    # unsort result from last part\n",
        "    local_index12 = rev_indices[sorted_local_index12]\n",
        "\n",
        "    # compute mapping between representation of central-other to pair\n",
        "    n = atom_index12.shape[1]\n",
        "    sign12 = ((local_index12 < n).astype(int) * 2) - 1\n",
        "    return central_atom_index, local_index12 % n, sign12\n",
        "\n",
        "def cumsum_from_zero(input_):\n",
        "    cumsum = np.zeros_like(input_)\n",
        "    np.cumsum(input_[:-1], axis=0, out=cumsum[1:])\n",
        "    return cumsum\n",
        "\n",
        "def triu_index(num_species):\n",
        "    species1, species2 = np.triu_indices(num_species,0, num_species)\n",
        "    pair_index = np.arange(species1.shape[0], dtype=np.int64)\n",
        "    ret = np.zeros(shape=(num_species, num_species), dtype=np.int64)\n",
        "    ret[species1, species2] = pair_index\n",
        "    ret[species2, species1] = pair_index\n",
        "    return ret\n",
        "\n",
        "\n",
        "def compute_aev(species, coordinates, triu_index, constants, sizes):\n",
        "    Rcr, EtaR, ShfR, Rca, ShfZ, EtaA, Zeta, ShfA = constants\n",
        "    num_species, radial_sublength, radial_length, angular_sublength, angular_length = sizes\n",
        "    num_molecules = species.shape[0]\n",
        "    num_atoms = species.shape[1]\n",
        "    num_species_pairs = angular_length // angular_sublength\n",
        "    coordinates_ = coordinates.reshape(-1, coordinates.shape[2])#possibly bad? make a soft copy\n",
        "    atom_index12 = neighbor_pairs_nopbc(species == -1, coordinates, Rcr)\n",
        "    selected_coordinates = np.take(coordinates_, atom_index12.flatten(), axis=0).reshape(2, -1, 3)\n",
        "    vec = selected_coordinates[0] - selected_coordinates[1]\n",
        "    species = species.flatten()\n",
        "    species12 = species[atom_index12]\n",
        "    distances = np.linalg.norm(vec,2, axis=-1)\n",
        "    # compute radial aev\n",
        "    radial_terms_ = radial_terms(Rcr, EtaR, ShfR, distances)\n",
        "    radial_aev = np.zeros((num_molecules * num_atoms * num_species, radial_sublength))\n",
        "    index12 = atom_index12 * num_species + np.flip(species12, axis=0)\n",
        "    radial_aev[index12[0].astype('int')] += radial_terms_\n",
        "    radial_aev[index12[1].astype('int')] += radial_terms_\n",
        "    radial_aev = radial_aev.reshape(num_molecules, num_atoms, radial_length)\n",
        "\n",
        "    # Rca is usually much smaller than Rcr, using neighbor list with cutoff=Rcr is a waste of resources\n",
        "    # Now we will get a smaller neighbor list that only cares about atoms with distances <= Rca\n",
        "    even_closer_indices = np.array((distances <= Rca).nonzero()).flatten()\n",
        "    atom_index12 = np.take(atom_index12, even_closer_indices, axis=1)\n",
        "    species12 = np.take(species12, even_closer_indices, axis=1)\n",
        "    vec = np.take(vec, even_closer_indices, axis=0)\n",
        "\n",
        "    # compute angular aev\n",
        "    central_atom_index, pair_index12, sign12 = triple_by_molecule(atom_index12)\n",
        "    species12_small = species12[:, pair_index12]\n",
        "    vec12 = np.take(vec, pair_index12.reshape(-1), axis=0).reshape(2, -1, 3) * np.expand_dims(sign12, axis=-1)\n",
        "    species12_ = np.where(sign12 == 1, species12_small[1], species12_small[0])\n",
        "    angular_terms_ = angular_terms(Rca, ShfZ, EtaA, Zeta, ShfA, vec12)\n",
        "    angular_aev = np.zeros((num_molecules * num_atoms * num_species_pairs, angular_sublength))\n",
        "    index = central_atom_index * num_species_pairs + triu_index[species12_[0].astype(int), species12_[1].astype(int)]\n",
        "    angular_aev[index] += angular_terms_\n",
        "    angular_aev = angular_aev.reshape(num_molecules, num_atoms, angular_length)\n",
        "    return np.concatenate([radial_aev, angular_aev], axis=-1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whiuWhszVEqm"
      },
      "source": [
        "coordinates = np.load('./coords_s01.npy')\n",
        "species = np.load('./species_s01.npy').astype(int)\n",
        "energies = np.load('./energies_s01.npy')\n",
        "aevs = np.array(compute_aev(species, coordinates, triu_index(4), constants, sizes))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQRowG5XWx8b"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ut8fb53ZCDT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b0f34b-68e8-43e6-8d88-3e43606738d7"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Input, Add, LeakyReLU, Reshape, Concatenate\n",
        "\n",
        "def create_submodel(layer_params, aev_length):\n",
        "  model = Sequential()\n",
        "  model.add(Input(shape=(aev_length,)))\n",
        "  for p in layer_params:\n",
        "    model.add(Dense(p))\n",
        "    model.add(LeakyReLU(0.1))\n",
        "  model.add(Dense(1))\n",
        "  return model\n",
        "\n",
        "H_params = [160,128,96]\n",
        "O_params = [128,112,96]\n",
        "N_params = [128,112,96]\n",
        "C_params = [144,112,96]\n",
        "modelH = create_submodel(H_params, aev_length)\n",
        "modelO = create_submodel(O_params, aev_length)\n",
        "modelN = create_submodel(N_params, aev_length)\n",
        "modelC = create_submodel(C_params, aev_length)\n",
        "models = [modelH, modelO, modelC, modelN]#the model list must be ordered this way // maybe define a dictionary?\n",
        "modelH(aevs[0,0,:].reshape(-1, aev_length))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.10864598]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzlneny-FrdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee9526ec-ae8a-47f8-d1a8-56312151059b"
      },
      "source": [
        "from tensorflow.keras.losses import MSE\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def train_model(model, species_aev, energies, epochs = 10, batch_size=2048, optimizer=Adam(), loss = MSE ):\n",
        "    num_mols, num_atoms, aev_length = species_aev[1].shape\n",
        "    species, aev = species_aev\n",
        "    full_batches = num_mols//batch_size\n",
        "    for epoch in range(epochs):\n",
        "        print(\"\\n Epoch %d\" % (epoch))\n",
        "        perm = np.arange(0, num_mols)\n",
        "        np.random.shuffle(perm)\n",
        "        batches = np.split(perm[num_mols % batch_size:], full_batches)\n",
        "        for b in tqdm(batches):\n",
        "            b = b.tolist()\n",
        "            with tf.GradientTape() as tape:\n",
        "                logits = testANI([species[b], aev[b]], training=True)\n",
        "                loss = MSE(energies[b], logits)\n",
        "                grads = tape.gradient(loss, testANI.trainable_weights)\n",
        "                optimizer.apply_gradients(zip(grads, testANI.trainable_weights))\n",
        "                print(\"Loss: %.4f\" % (np.mean(loss)))\n",
        "          \n",
        "\n",
        "class ANImodel(tf.keras.Model):\n",
        "    def __init__(self, model_list, name=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.model_list = model_list\n",
        "    \n",
        "    def call(self, species_aev):#entelws la8os // pws diaxeirizomaste to batch_size sto axis=0?\n",
        "        species, aev = species_aev\n",
        "        mol_num = species.get_shape()[0]\n",
        "        atom_num = species.get_shape()[1]\n",
        "        aev_len = aev.get_shape()[2]\n",
        "        aev_species_list = tf.split(aev, atom_num, axis=1)#list of tensors (conform, 1, aev_len), of atoms\n",
        "        species_ = species.numpy().astype(int)\n",
        "        for i_s,a in enumerate(aev_species_list):\n",
        "            a = tf.squeeze(a)# -> (conform, aev_len)\n",
        "            a_list = tf.split(a, mol_num)# list of tensors (1, aev_len), of conforms\n",
        "            #print(a_list)\n",
        "            for i_m,b in enumerate(a_list):\n",
        "                if species_[i_m,i_s] == -1:\n",
        "                    a_list[i_m] = tf.zeros((1,1))\n",
        "                else:\n",
        "                    a_list[i_m] = self.model_list[species_[i_m,i_s].astype(int)](b)#-> list of tensors (1,1), of conforms\n",
        "            aev_species_list[i_s] = Concatenate()(a_list)\n",
        "        conform_energies = Add()(aev_species_list)\n",
        "        return conform_energies\n",
        "      \n",
        "testANI = ANImodel(models, name=\"testANI\")\n",
        "train_model(testANI, [species, aevs], energies, epochs=1)#training is insufferably slow // maybe reduce AEV size? "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:33<02:12, 33.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 2845.2188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [01:07<01:40, 33.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 2763.2654\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 3/5 [01:40<01:06, 33.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 2754.3694\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 4/5 [02:14<00:33, 33.42s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 2570.7188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [02:48<00:00, 33.62s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 2459.9841\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNN6cLSpv3W1",
        "outputId": "a90d3da8-eabf-45b8-c5cb-6f2cfd3629a5"
      },
      "source": [
        "from tensorflow.keras.losses import MSE\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def train_model(model, species_aev, energies, epochs = 10, batch_size=2048, optimizer=Adam(), loss = MSE ):\n",
        "    num_mols, num_atoms, aev_length = species_aev[1].shape\n",
        "    species, aev = species_aev\n",
        "    full_batches = num_mols//batch_size\n",
        "    for epoch in range(epochs):\n",
        "        print(\"\\n Epoch %d\" % (epoch))\n",
        "        perm = np.arange(0, num_mols)\n",
        "        np.random.shuffle(perm)\n",
        "        batches = np.split(perm[num_mols % batch_size:], full_batches)\n",
        "        for b in tqdm(batches):\n",
        "            b = b.tolist()\n",
        "            with tf.GradientTape() as tape:\n",
        "                logits = testANI([species[b], aev[b]], training=True)\n",
        "                loss = MSE(energies[b], logits)\n",
        "                grads = tape.gradient(loss, testANI.trainable_weights)\n",
        "                optimizer.apply_gradients(zip(grads, testANI.trainable_weights))\n",
        "                print(\"Loss: %.4f\" % (np.mean(loss)))\n",
        "                \n",
        "        \n",
        "        \n",
        "class ANImodel_fast(tf.keras.Model):\n",
        "    def __init__(self, model_list, name=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.model_list = model_list\n",
        "    \n",
        "    def call(self, species_aev):#entelws la8os // pws diaxeirizomaste to batch_size sto axis=0?\n",
        "        species, aev = species_aev\n",
        "        mol_num = species.get_shape()[0]\n",
        "        atom_num = species.get_shape()[1]\n",
        "        aev_len = aev.get_shape()[2]\n",
        "        aev_ = tf.reshape(aev, (mol_num*atom_num, aev_len))#flatten the first two dimensions of the aevs\n",
        "        species_ = species.numpy().flatten().astype(int)#completely flatten the species array\n",
        "        output = tf.zeros(mol_num*atom_num)#initialize output\n",
        "        HOCN_energies = []#list of tensors that will contain the energies predicted by each model\n",
        "        for i, m in enumerate(self.model_list):\n",
        "            mask = ( species_ == i)#mask to pass the correct aevs through the right models\n",
        "            midx = mask.nonzero()\n",
        "            if (len(midx[0])) > 0:\n",
        "                midx = tf.convert_to_tensor(midx)#indices of the elements to pass through\n",
        "                input_ = tf.gather(aev_, midx, axis=0)#aevs to pass through the models\n",
        "                input_ = tf.squeeze(input_)\n",
        "                #print(input_.get_shape())\n",
        "                #print(midx.get_shape())\n",
        "                #print(output.get_shape())\n",
        "                #print(indices.shape[:-1] + shape[indices.shape[-1]:])\n",
        "                output_ = m(input_)\n",
        "                #print(output_.get_shape())\n",
        "                HOCN_energies.append(tf.scatter_nd(tf.reshape(midx, (-1,1)), tf.reshape(output_, shape=(-1)), output.shape))\n",
        "            #broadcasts a batch of aevs through the model, then puts them in a collective tensor which is then appended to the list of energies\n",
        "        output = tf.math.add_n(HOCN_energies)#the collective output is the element-wise sum of tensors in the list\n",
        "        output = tf.reshape(output, species.get_shape())#return to previous view\n",
        "        output_list = tf.split(output, atom_num, axis=1)#splits the output into an atom_num len list, wherein each element is a (num_conform, 1) tensor\n",
        "        output = Add()(output_list)#add the tensors in the list\n",
        "        return output #output is now (nol_num x 1)\n",
        "      \n",
        "testANI = ANImodel_fast(models, name=\"testANI\")\n",
        "testANI([species[0:2].astype(int), aevs[0:2]])\n",
        "train_model(testANI, [species, aevs], energies, epochs=1)#training is insufferably slow // maybe reduce AEV size? "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:00<00:01,  3.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 2448.5068\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [00:00<00:00,  4.04it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 2227.2151\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 3/5 [00:00<00:00,  4.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 2142.0435\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 4/5 [00:00<00:00,  4.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 1964.3042\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:01<00:00,  4.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 1750.4697\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sgxVAPt5D7U",
        "outputId": "193b6056-3537-47aa-e19f-dd754eeaad5a"
      },
      "source": [
        "testANI.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"an_imodel_fast\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential (Sequential)      (None, 1)                 235489    \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 1)                 187313    \n",
            "_________________________________________________________________\n",
            "sequential_3 (Sequential)    (None, 1)                 209345    \n",
            "_________________________________________________________________\n",
            "sequential_2 (Sequential)    (None, 1)                 187313    \n",
            "=================================================================\n",
            "Total params: 819,460\n",
            "Trainable params: 819,460\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu2-QhJowmvu"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}